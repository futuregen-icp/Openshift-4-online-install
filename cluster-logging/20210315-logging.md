
## 목적
- systemd journal 및 /var/log/containers/ 의 모든 로그를 Elasticsearch 에 제공한다.

## 수집기 동작 아키텍처
- 따라서 미리 필요한 데이터 양과 애플리케이션 로그 데이터를 집계하는 것을 고려해야 함
- 일부 Elasticsearch 사용자는 절대 스토리지 소비량을 항상 약 50%에서 70% 미만으로 유지해야 한다
- 이는 대규모 병합 작업 중에 Elasticsearch 가 응답하지 않는 것을 방지 한다.

- 기본적으로 85%에서 Elasticsearch는 새 데이터를 노드에 할당하는 것을 중지 하고 ,
- 90%에서 Elasticsearch 는 기존 shard를 가능한 경우 해당 노드에서 다른 노드로 재배치하려고 시도합니다. 그러나 사용 가능한 용량이 85%미만인 노드가 없는 경우 Elasticsearch는 새 인덱스 생성을 효과적으로 거부하고 red가 된다.

- 참고) 이 낮은 워크마크 값과 높은 어쿼마크 값은 현재 릴리스에서 Elasticsearch기본값입니다.

## 웹 콘솔을 사용하여 클러스터 로깅 설치
- Openshift Contianer Platform 웹 콘솔을 사용하여 Elasticsearch 및 클러스터 로깅 Operator를 설치할 수 있습니다.

+ 사전 요구 사항
  - Elasticsearch 필요한 영구 스토리지가 있는지 확인하십시오. 각 Elasticsearch 노드에서 각각 스토리지가 필요 합니다.
  - Elasticsearch는 메모리를 많이 사용하는 애플리케이션입니다. 기본적으로 Openshift Container Platform은 메모리 요청 및 제한이 16G인 3개의 Elasricsearch 노드를 설치합니다. 
    이 초기 3개의 Openshift Container Platform 노드 세트에서 클러스터 내에서 Elasticsearch를 실행하기에 충분한 메모리가 없을 수 있습니다.
    Elasticsearch와 관련된 메모리 문제가 발생하는 경우 기존 노드의 메모리를 늘리는 대신 클러스터에 Elasticsearch 노드를 더 추가해야 합니다.

+ 구성절차
  1. Elasticsearch Operator 설치
    a. openshift-operaters-redhat 네임스페이스 생성
    a. Operator > Operator hub클릭
    b. 사용 가능한 Operator 목록에서 Elasticsearch Operator 설치

+ CRD * 이름은 instance 이어야 합니다.
```
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
  namespace: "openshift-logging"
spec:
  managementState: "Managed"
  logStore:
    type: "elasticsearch"
    retentionPolicy:
      application:
        maxAge: 1d
      infra:
        maxAge: 7d
      audit:
        maxAge: 7d
    elasticsearch:
      nodeCount: 3
      storage:
        storageClassName: ""
        size: 200G
      redundancyPolicy: "SingleRedundancy"
  visualization:
    type: "kibana"
    kibana:
      replicas: 1
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *"
  collection:
    logs:
      type: "fluentd"
      fluentd: {}
```

+ 주요 키워드
  - 특정 노드로 이동
  - 사용자 정의 리소스 cr의 spec,collection.logfluentd 스탠자를 통해 로그 수집기에 대해 지원되는 모든 수정을 수행할 수 있습니다.

+ Fluentd
  - Fluentd는 정크라는 단일 blob에서 로그 데이터를 수집합니다. Fluentd가 정크를 생성할 때 정크는 스테이지에 있는 것으로 간주되어 정크가 데이터로 채워집니다.
    정크가 가득 차면 Fluentd는 정크를 큐로 이동합니다. 여기서 정크는 플러시되기 전에 보관되거나 대상에 기록됩니다. Fluentd는 네트워크 문제 또는 대상의 용량문제와 같은 여러가지 이유로 정크를 플러시하지 못할 수 있습니다. 
    정크를 플러시할 수 없는 경우 Fluentd는 구서오딘대로 플러시를 다시 시도합니다.
    기본적으로 Openshift Container Platform 에서 Fluentd는 지수 백오프 방법을 사용하여 플러시를 다시 시도 합니다. 여기서 Fluentd는 다시 플러시를 다시 시도하는 사이에 대기하는 시간을 두 배포 늘려 대상에 대한 연결 요청을 줄이는 데 도움이 됩니다. 지수 백오프를 비활성화하고 대신 주기적 재시도 방법을 사용하여 지정된 간격으로 정크 플러시를 재시도 할 수 있습니다. 기본적으로 Fluentd는 정크 플러싱을 무기한 재 시도 합니다. Openshift Container Platform에서는 무제한 재시도 동작을 변경할 수 있습니다.
    이러한 매개변수는 대기 시간과 처리량 간의 균형을 경정하는 데 도움이 될 수 있습니다.
    + 처리량에 대해 Fluend를 최적화하려면 이러한 매개변수를 사용하여 더 큰 버퍼 및 큐를 구성하고, 플러시를 지연하고, 재시도 간격을 더 길게 설정하여 네트워크 패킷 수를 줄일 수 있습니다. 버퍼가 클수록 노드 파일 시스템에 더 많은 공간이 필요합니다.
    + 짧은 지연 시간을 최적화하기 위해 매개변수를 사용하여 가능한 빨리 데이터를 전송하고, 배치축적을 방지하고, 큐와 버퍼를 더 짧게 만들고, 더 자주 플러시 및 재시도를 사용할 수 있습니다.
    
+ 로그 저장소 구성
  - Elasticsearch 클러스터의 스토리지
  - shard가 전체 복제에서 복제 없이 클러스터의 데이터 노드에 복제되는 방법
  - Elasticsearch 데이터에 대한 외부 엑세스 허용합니다.
    * 참고) Elasticsearch 노드 축소는 지원되지 않습니다. 축소하면 Elasticsearch포드가 실수로 삭제되어 shard가 할당되지 않고 복제 shard가 손실될 수 있습니다.
+ 감사 로그를 로그 저장소로 전달
  - 내부 Openshift Container Platform Elasticsearch 로그 저장소는 감사 로그를 위한 스토리지를 제공하지 않기 때문에 기본적으로 감사 로그는 내부 Elasticsearch 인스턴스에 저장되지 않습니다.
    예를 들어 Kibana 에서 감사로그를 보기위해 감사로그를 내부 로그 저장소로 보내려면 Log Forward API를 사용해야 합니다.
    * 중요) 내부 Openshift Container Platform Elasticsearch 로그저장소는 감사 로그를 위한 보안 스토리지를 제공하지 않습니다. 감사 로그를 전달하는 시스템이 조직 및 정부 규정을 준수하고 올바를게 보호되도록 하는 것이 좋습니다. Openshift Container Platform 클러스터 로깅은 이러한 규정을 준수하지 않습니다.


+ Shard 에 대한 중복 정책을 지정합니다. 변경사항을 저장하면 변경 사항이 적용됩니다.  
  - FullRedundancy: Elasticsearch는 각 인텍스의 기본 shard를 데이터 노드에 완전히 복제합니다. 이는 가장 높은 안정성을 제공하지만 필요한 디스크 양이 가장 많고 성능이 가장 낮습니다.
  - MultipleRedundancy: Elasticsearch는 각 인덱스의 기본 shard를 데이터 노드의 절반으로 완전히 복제합니다. 이는 안전성과 성능 사이의 좋은 균형을 제공합니다.
  - SingleRedundancy: Elasticsearch는 각 인덱스에 대한 기본 shard의 사본 하나를 만듭니다. 두 개 이상의 데이터 노드가 존재하는 한 항상 로그를 사용할 수 있고 복구할 수 있습니다. 5개 이상의 노드를 사용하는 경우 MultipleRedundancy보다 성능이 향상됩니다. 단일 Elasticsearch 노드 배포에는 이 정책을 적용할 수 없습니다.
  - ZeroRedundancy: Elasticsearch는 기본 shard의 사본을 만들지 않습니다. 노드가 다운되거나 실패한 경우 로그를 사용할 수 없거나 손실될 수 있습니다. 안전보다 성능이 더 중요하거나 자체 디스크/PVC백업/복원 전략을 구현할 경우 이 모드를 사용합니다. 

+ 로그 저장소에 대한 영구 스토리지 구성
  - aws op2 경우
```
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
  ....
  spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        nodeCount: 3
        storage:
          storageClassName: "gp2"
          size: "200G"
 ```
  - emptyDir
```
  spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        nodeCount: 3
        storage: {}

```
+ Elasticsearch 롤링 클러스터 재시작 수행
  - elasticsearch 구성맵 또는 elasticsearch-* 배치 구성을 변경할 때 롤링 재시작을 수행합니다.
    또한 Elasticsearch 포드가 실행되는 노드를 재부팅해야 하는 경우 롤링 재시작이 권장 됩니다.


