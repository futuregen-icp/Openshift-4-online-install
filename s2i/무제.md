# **Authentication**



## Local Password and LDAP Authentication Configuration Lab

In this lab, you configure Red Hat® OpenShift® Container Platform 4 to use local passwords with an HTPasswd identity provider, configure role based access controls, and disable the initial `kubeadmin` user.

Goals

- Configure an HTPasswd identity provider
- Configure an LDAP identity provider
- Disable the `kubeadmin` user
- Troubleshoot authentication issues

|      | You can store files created in this lab in any convenient location. You can work directly from the home directory of your user on the bastion host or create a folder for lab files as you prefer. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Many of the `oc` commands in this lab use the `system:admin` user by explicitly referencing the `kubeconfig` file created during initial OpenShift cluster installation. This is done by using the `‑‑config` command line option. This is considered a best practice because `system:admin` can still be used if OAuth authentication is unavailable. If you are working from a system other than the bastion cluster then you must copy the `kubeconfig` to your host. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

For this lab, you must provision an OpenShift Container Platform 4 lab environment. If you have not already done so, perform the environment setup lab in the first module of this course.

## 1. Configure Access to `system:admin` User

In this section, you set up cluster admin access to configure OpenShift authentication.

1. Copy the `kubeconfig` created by the OpenShift Container Platform installation to your user’s `$HOME/.kube/config` location:

   ```sh
   $ cp $HOME/cluster-$GUID/auth/kubeconfig $HOME/.kube/config
   ```

2. Confirm that your kubeconfig `admin` user corresponds to the `system:admin` cluster account:

   ```sh
   $ oc --user=admin whoami
   system:admin
   ```

## 2. Configure Local Password Identity Provider

The identity provider for OpenShift that is simplest to use is the HTPasswd identity provider, which uses user passwords stored in the cluster `etcd` storage as a secret. In this section, you configure an HTPasswd identity provider.

### 2.1. Create HTPasswd Secret

You start by configuring a local secret called `htpasswd` in the `openshift-config` namespace containing the users `alice`, `bob`, and `claire`. You set the password for each user to `p4ssw0rd`.

1. Create an empty file called `htpasswd`:

   ```sh
   $ touch htpasswd
   ```

2. Use `htpasswd` to set the password for the `alice`, `bob`, and `claire` users to `p4ssw0rd`:

   ```sh
   $ htpasswd -Bb htpasswd alice p4ssw0rd
   $ htpasswd -Bb htpasswd bob p4ssw0rd
   $ htpasswd -Bb htpasswd claire p4ssw0rd
   ```

3. Create the `htpasswd` secret from the `htpasswd` file in the `openshift-config` namespace:

   ```sh
   $ oc --user=admin create secret generic htpasswd \
       --from-file=htpasswd -n openshift-config
   ```

### 2.2. Configure OAuth Identity Provider for HTPasswd and Test

1. Create a file called `$HOME/oauth-config.yaml` with the following contents:

   ```yaml
   apiVersion: config.openshift.io/v1
   kind: OAuth
   metadata:
     name: cluster
   spec:
     identityProviders:
     - name: Local Password
       mappingMethod: claim
       type: HTPasswd
       htpasswd:
         fileData:
           name: htpasswd
   ```

2. Replace the cluster OAuth configuration with the HTPasswd version in the `oauth-config.yaml` file:

   ```sh
   $ oc --user=admin replace -f oauth-config.yaml
   oauth.config.openshift.io/cluster replaced
   ```

3. Retrieve the web console URL, open a browser window, paste the web console URL, and test the login with the `Local Password` identity provider, using `alice` as the user and `p4ssw0rd` as the password:

   ```sh
   $ oc whoami --show-console
   ```

   |      | You may need to access a self-signed certificate twice—once for the web console and once for the OAuth endpoint. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

4. In the top right corner, click the username, then click **Logout** to log out of your web browser.

5. Retrieve the API URL and test the login using the command line, specifying `bob` as the user and `p4ssw0rd` as the password:

   ```sh
   $ API_URL=$(oc whoami --show-server)
   $ oc login -u bob -p p4ssw0rd $API_URL
   Login successful.
   
   You dont have any projects. You can try to create a new project, by running
   
       oc new-project <projectname>
   
   $ oc whoami
   bob
   ```

6. List the users and their identities to view the dynamically created entries:

   ```sh
   $ oc --user=admin get users
   NAME    UID                                    FULL NAME   IDENTITIES
   alice   899e9805-81c2-11e9-8c05-0a580a80001a               Local Password:alice
   bob     a14c7553-81b4-11e9-8c05-0a580a80001a               Local Password:bob
   
   $ oc --kubeconfig $HOME/cluster-$GUID/auth/kubeconfig get identities
   NAME                  IDP NAME        IDP USER NAME  USER NAME  USER UID
   Local Password:bob    Local Password  bob            bob        a14c7553-81b4-11e9-8c05-0a580a80001a
   Local Password:alice  Local Password  alice          alice      899e9805-81c2-11e9-8c05-0a580a80001a
   ```

   |      | No user or identity for `claire` appears because user objects are created on first login. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

## 3. Configure LDAP Identity Provider

In this section, you configure the LDAP authentication against the OPENTLC IPA LDAP infrastructure.

### 3.1. Configure LDAP TLS Certificate Authority ConfigMap and Bind Password Secret

In this section you download the OPENTLC IPA TLS certificate authority (CA) certificate and store it in a ConfigMap in the `openshift-config` namespace.

The OPENTLC IPA also requires LDAP bind credentials to perform searches for user accounts. The bind user is configured as the `bindDN` in the identity provider configuration. The bind password, `r3dh4t1!`, must be stored as a secret using the `bindPassword` data key.

1. Download the OPENTLC IPA TLS certificate authority certificate:

   ```sh
   $ curl http://ipa.shared.example.opentlc.com/ipa/config/ca.crt -o ipa-ca.crt
     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                    Dload  Upload   Total   Spent    Left  Speed
   100  1350  100  1350    0     0   6597      0 --:--:-- --:--:-- --:--:--  6617
   ```

2. Create the `opentlc-ipa-tls-ca` ConfigMap:

   ```sh
   $ oc --user=admin create configmap -n openshift-config \
       opentlc-ipa-tls-ca --from-file=ca.crt=ipa-ca.crt
   configmap/opentlc-ipa-tls-ca created
   ```

3. Create the `opentlc-ipa-bind-password` secret:

   ```sh
   $ oc --user=admin create secret generic -n openshift-config \
       opentlc-ipa-bind-password --from-literal=bindPassword=r3dh4t1!
   secret/opentlc-ipa-bind-password created
   ```

### 3.2. Configure and Test LDAP Identity Provider

1. Edit the `$HOME/oauth-config.yaml` file and add the LDAP identity provider configuration using these settings:

   | Field                               | Explanation                                                  |
   | :---------------------------------- | :----------------------------------------------------------- |
   | `ldap.attributes.email`             | User email address configured as `mail`                      |
   | `ldap.attributes.id`                | User identity in LDAP configured as `dn`                     |
   | `ldap.attributes.name`              | Name attribute configured as `cn`                            |
   | `ldap.attributes.preferredUsername` | Preferred username configured as `uid`                       |
   | `ldap.bindDN`                       | LDAP bind user DN for user search                            |
   | `ldap.bindPassword.name`            | OpenShift secret name configured with bind password for LDAP user search bind |
   | `ldap.ca.name`                      | OpenShift `ConfigMap` name configured with `ca.crt` for OPENTLC IPA |
   | `ldap.insecure`                     | Configured for secure communication with TLS encryption      |
   | `ldap.url`                          | LDAP user search URL with base DN location for user accounts, username attribute set to `uid`, searching the LDAP subtree under the base DN, and filter to restrict users to only those that are member of specific group |
   | `name`                              | Identity provider name appears as `OPENTLC LDAP`             |
   | `type`                              | `LDAP` value indicates LDAP identity provider                |

   Example YAML File

   ```yaml
   apiVersion: config.openshift.io/v1
   kind: OAuth
   metadata:
     name: cluster
   spec:
     identityProviders:
     - name: Local Password
       mappingMethod: claim
       type: HTPasswd
       htpasswd:
         fileData:
           name: htpasswd
     - name: OPENTLC LDAP
       challenge: true
       login: true
       mappingMethod: claim
       type: LDAP
       ldap:
         attributes:
           email: ["mail"]
           id: ["dn"]
           name: ["cn"]
           preferredUsername: ["uid"]
         bindDN: "uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com"
         bindPassword:
           name: opentlc-ipa-bind-password
         insecure: false
         ca:
           name: opentlc-ipa-tls-ca
         url: "ldaps://ipa.shared.example.opentlc.com:636/cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com?uid?sub?(memberOf=cn=ocp-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com)" 
   ```

2. Replace the OAuth configuration with the LDAP version in the `oauth-config.yaml` file:

   ```sh
   $ oc --user=admin replace -f oauth-config.yaml
   oauth.config.openshift.io/cluster replaced
   ```

3. Retrieve the web console URL again, return to your web browser:

   ```sh
   $ oc whoami --show-console
   ```

4. In the top right corner, click the username `alice`, then click **Logout** to log out.

5. Test the login in using the `OPENTLC LDAP` identity provider, using the LDAP `andrew` user and `r3dh4t1!` as the password.

6. Retrieve the API URL and test the login as the `karla` user:

   ```sh
   $ API_URL=$(oc whoami --show-server)
   $ oc login -u karla -p r3dh4t1! $API_URL
   Login successful.
   
   You dont have any projects. You can try to create a new project, by running
   
       oc new-project <projectname>
   
   $ oc whoami
   karla
   ```

7. List the users and their identities to dynamically view the created entries:

   ```sh
   $ oc --user=admin get users
   NAME    UID                                   FULL NAME       IDENTITIES
   alice   899e9805-81c2-11e9-8c05-0a580a80001a                  Local Password:alice
   andrew  73700fef-828a-11e9-8c05-0a580a80001a  andrew OSEUser  OPENTLC LDAP:dWlkPWFuZHJldyxjbj11c2Vycyxjbj1hY2NvdW50cyxkYz1zaGFyZWQsZGM9ZXhhbXBsZSxkYz1vcGVudGxjLGRjPWNvbQ
   bob     a14c7553-81b4-11e9-8c05-0a580a80001a                  Local Password:bob
   karla   6ef11416-828a-11e9-b2d0-0a580a810023  karla OSEUser   OPENTLC LDAP:dWlkPWthcmxhLGNuPXVzZXJzLGNuPWFjY291bnRzLGRjPXNoYXJlZCxkYz1leGFtcGxlLGRjPW9wZW50bGMsZGM9Y29t
   
   $ oc --user=admin get identities
   NAME                                                                                                         IDP NAME        IDP USER NAME                                                                                   USER NAME  USER UID
   Local Password:bob                                                                                           Local Password  bob                                                                                             bob        a14c7553-81b4-11e9-8c05-0a580a80001a
   Local Password:alice                                                                                         Local Password  alice                                                                                           alice      899e9805-81c2-11e9-8c05-0a580a80001a
   OPENTLC LDAP:dWlkPWFuZHJldyxjbj11c2Vycyxjbj1hY2NvdW50cyxkYz1zaGFyZWQsZGM9ZXhhbXBsZSxkYz1vcGVudGxjLGRjPWNvbQ  OPENTLC LDAP    dWlkPWFuZHJldyxjbj11c2Vycyxjbj1hY2NvdW50cyxkYz1zaGFyZWQsZGM9ZXhhbXBsZSxkYz1vcGVudGxjLGRjPWNvbQ  andrew     73700fef-828a-11e9-8c05-0a580a80001a
   OPENTLC LDAP:dWlkPWthcmxhLGNuPXVzZXJzLGNuPWFjY291bnRzLGRjPXNoYXJlZCxkYz1leGFtcGxlLGRjPW9wZW50bGMsZGM9Y29t    OPENTLC LDAP    dWlkPWthcmxhLGNuPXVzZXJzLGNuPWFjY291bnRzLGRjPXNoYXJlZCxkYz1leGFtcGxlLGRjPW9wZW50bGMsZGM9Y29t    karla      6ef11416-828a-11e9-b2d0-0a580a810023
   ```

## 4. Disable `kubeadmin` Account

Because you have direct access as the `system:admin` account using the `kubeconfig` installer file, you do not need the `kubeadmin` account to be active in the cluster. In this section, you disable the `kubeadmin` account by removing the password secret.

1. Delete the `kubeadmin` secret from the `kube-system` namespace:

   ```sh
   $ oc --user=admin delete secret kubeadmin -n kube-system
   secret "kubeadmin" deleted
   ```

2. Confirm that the `kubeadmin` user is no longer accessible:

   ```sh
   $ API_URL=$(oc whoami --show-server)
   $ KUBEADMIN_PASSWORD="$(cat $HOME/cluster-$GUID/auth/kubeadmin-password)"
   $ oc login -u kubeadmin -p "$KUBEADMIN_PASSWORD" "$API_URL"
   Login failed (401 Unauthorized)
   Verify you have provided correct credentials.
   ```

3. Use TLS authentication to confirm that `system:admin account` is still available:

   ```sh
   $ oc --user=admin whoami
   system:admin
   ```





# **Group Management** 

## Group Management Lab

In this lab, you configure groups and LDAP group synchronization on your Red Hat® OpenShift® Container Platform 4 cluster.

Goals

- Manually create an OpenShift user group
- Configure LDAP group synchronization
- Perform OpenShift group synchronization with an LDAP server
- Automate LDAP group synchronization with an OpenShift cron job

Prerequisites

This lab assumes that you completed the previous lab and that you have the local HTPasswd and OPENTLC LDAP identity providers available from that lab.

## 1. Manually Configure Groups

In this section, you manually define a `local-admin` group. Because you disabled the `kubeadmin` user in a previous lab, these next steps must be performed using the `system:admin` user account.

### 1.1. Create `local-admin` Group

1. Create a new group called `local-admin`:

   ```sh
   $ oc --user=admin adm groups new local-admin
   group.user.openshift.io/local-admin created
   ```

2. Add the `alice` user to the `local-admin` group:

   ```sh
   $ oc --user=admin adm groups add-users local-admin alice
   group.user.openshift.io/local-admin added: "alice"
   ```

3. Confirm the group configuration:

   ```sh
   $ oc --user=admin get groups
   NAME          USERS
   local-admin   alice
   ```

## 2. Configure LDAP Group Synchronization

In this section, you synchronize the following groups from the IPA server to your OpenShift cluster and verify their synchronization:

- `group/portalapp`
- `group/paymentapp`
- `group/ocp-production`
- `group/ocp-platform`

You use the following information to synchronize the groups:

| Variable                     | Value                                                        |
| :--------------------------- | :----------------------------------------------------------- |
| `bindDN`                     | `uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com` |
| `bindPassword`               | `r3dh4t1!`                                                   |
| `ca`                         | `/etc/origin/master/ipa-ca.crt`                              |
| `url`                        | `ldap://ipa.shared.example.opentlc.com or ldaps://ipa.shared.example.opentlc.com:636` |
| `rfc2307.groupsQuery.baseDN` | `cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com` |
| `rfc2037.usersQuery.baseDN`  | `cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com` |

|      | LDAP entries are referenced by distinguished name (DN)--for example, `cn=portalapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com`. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.1. Explore LDAP Entries

In this section, you search LDAP for user and group information to determine which groups are appropriate for OpenShift group synchronization.

1. Download the OPENTLC IPA TLS certificate authority bundle if it is not already present in your current directory:

   ```sh
   $ curl http://ipa.shared.example.opentlc.com/ipa/config/ca.crt -o $HOME/ipa-ca.crt
   ```

2. Set the `LDAPTLS_CACERT` environment variable so that `ldapsearch` can verify the validity of the LDAP server’s TLS certificate:

   ```sh
   $ export LDAPTLS_CACERT=$HOME/ipa-ca.crt
   ```

3. Search the `groups` base DN to discover the entries there:

   ```sh
   $ ldapsearch -Z \
       -D uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com \
       -w r3dh4t1! \
       -h ipa.shared.example.opentlc.com \
       -b cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   # extended LDIF
   #
   # LDAPv3
   # base <cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com> with scope subtree
   # filter: (objectclass=*)
   # requesting: ALL
   #
   
   ... output omitted ...
   
   # ocp-users, groups, accounts, shared.example.opentlc.com
   dn: cn=ocp-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   objectClass: top
   objectClass: groupofnames
   objectClass: nestedgroup
   objectClass: ipausergroup
   objectClass: ipaobject
   objectClass: posixgroup
   cn: ocp-users
   description: Users with OpenShift access
   ipaUniqueID: 27879178-49b8-11e9-8679-001a4a16015a
   gidNumber: 1354800001
   member: uid=andrew,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=marina,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=karla,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=david,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=portal1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=portal2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=payment1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=payment2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=prod1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=prod2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=platform1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=platform2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=admin1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=admin2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   
   ... output omitted ...
   
   # search result
   search: 3
   result: 0 Success
   
   # numResponses: 27
   # numEntries: 26
   ```

   |      | See `man ldapsearch` or `ldapsearch --help` for reference on the command line options used. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

4. In the results, note the relevant entries that have a common property of `objectClass: groupofnames`, search specifically on this, and `grep` for just the DN field:

   ```sh
   $ ldapsearch -Z \
       -D uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com \
       -w r3dh4t1! \
       -h ipa.shared.example.opentlc.com \
       -b cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\
       '(objectClass=groupofnames)' \
     | grep '^dn:'
   dn: cn=admins,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=ipausers,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=editors,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=trust admins,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=c
   dn: cn=ocp-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=portalapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=paymentapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   dn: cn=ocp-production,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc
   dn: cn=ocp-platform,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=c
   dn: cn=openshift-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,d
   ```

5. Search the `cn` to examine one of these specific groups:

   ```sh
   $ ldapsearch -Z \
       -D uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com \
       -w r3dh4t1! \
       -h ipa.shared.example.opentlc.com \
       -b cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\
       '(cn=ocp-platform)'
   # extended LDIF
   #
   # LDAPv3
   # base <cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com> with scope subtree
   # filter: cn=ocp-platform
   # requesting: ALL
   #
   
   # ocp-platform, groups, accounts, shared.example.opentlc.com
   dn: cn=ocp-platform,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=c
    om
   objectClass: top
   objectClass: groupofnames
   objectClass: nestedgroup
   objectClass: ipausergroup
   objectClass: ipaobject
   objectClass: posixgroup
   cn: ocp-platform
   description: Users with full cluster administration control
   ipaUniqueID: 2a09f3dc-49b8-11e9-a954-001a4a16015a
   gidNumber: 1354800006
   member: uid=david,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=admin1,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=admin2,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   member: uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   memberOf: cn=openshift-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=ope
    ntlc,dc=com
   
   # search result
   search: 3
   result: 0 Success
   
   # numResponses: 2
   # numEntries: 1
   ```

6. Make note of the following:

   - `groupofnames` is a suitable search filter, but only the groups `ocp-users`, `portalapp`, `paymentapp`, `ocp-production`, and `ocp-platform` appear to be appropriate for group sync. You must include the DNs for these entries in a whitelist.
   - Either `dn` or `ipaUniqueID` appears to be suitable for a unique group identifier. You use `dn` because it is more suitable for whitelist usage to have easily identifiable entries in the whitelist file.
   - The group name is in the `cn` field.
   - The `member` field lists the users that belong to the group.
   - As shown in the `member` listing, the user `uid` field provides the user name.

### 2.2. Configure Group Synchronization

1. Create a file called `$HOME/groupsync.yaml` informed by the exploration with `ldapsearch`:

   Example YAML File

   ```yaml
   kind: "LDAPSyncConfig"
   apiVersion: "v1"
   url: "ldap://ipa.shared.example.opentlc.com"
   insecure: false
   bindDN: "uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com"
   bindPassword: "r3dh4t1!"
   ca: "ipa-ca.crt"
   rfc2307:
     groupsQuery:
       baseDN: "cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com"
       derefAliases: "never"
       filter: "(objectClass=groupofnames)"
     groupUIDAttribute: "dn"
     groupNameAttributes: ["cn"]
     groupMembershipAttributes: ["member"]
     usersQuery:
       baseDN: "cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com"
       derefAliases: "never"
     userNameAttributes: ["uid"]
     userUIDAttribute: "dn"
   ```

2. Create a file called `$HOME/whitelist.txt` with the DNs of the five OpenShift groups:

   ```texinfo
   cn=ocp-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   cn=portalapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   cn=paymentapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   cn=ocp-production,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   cn=ocp-platform,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com
   ```

### 2.3. Run and Verify Group Synchronization

1. Perform a test run of the synchronization:

   ```sh
   $ oc --user=admin adm groups sync \
       --sync-config=groupsync.yaml \
       --whitelist=whitelist.txt
   items:
   - metadata:
       annotations:
         openshift.io/ldap.sync-time: "2019-05-30T18:56:33Z"
         openshift.io/ldap.uid: 28080a1a-49b8-11e9-93a6-001a4a16015a
         openshift.io/ldap.url: ipa.shared.example.opentlc.com:389
       creationTimestamp: null
       labels:
         openshift.io/ldap.host: ipa.shared.example.opentlc.com
       name: portalapp
     users:
     - andrew
     - portal1
     - portal2
   ... output omitted ...
   kind: List
   metadata: {}
   ```

2. Run the same `oc adm groups sync` command, but this time add `--confirm` to create the groups:

   ```sh
   $ oc --user=admin adm groups sync \
       --sync-config=groupsync.yaml \
       --whitelist=whitelist.txt \
       --confirm
   group/ocp-users
   group/portalapp
   group/paymentapp
   group/ocp-production
   group/ocp-platform
   ```

3. Verify that the groups are created:

   ```sh
   $ oc --user=admin get groups
   NAME             USERS
   local-admin      alice
   ocp-platform     david, admin1, admin2, admin
   ocp-production   karla, prod1, prod2, admin, redhat
   ocp-users        andrew, marina, karla, david, portal1, portal2, payment1, payment2, prod1, prod2, platform1, platform2, admin1, admin2, admin
   paymentapp       marina, payment1, payment2
   portalapp        andrew, portal1, portal2
   ```

   - The groups may reference users that do not yet exist in the cluster.
   - Users are created dynamically upon first OAuth login.

### 2.4. Configure LDAP Group Synchronization Cron Job

In this section, you configure LDAP group sync to run periodically. You can do this with external automation or from within the cluster. Red Hat Communities of Practice provides an OpenShift template for configuring [LDAP Group Synchronization](https://github.com/redhat-cop/openshift-management/tree/master/jobs#ldap-group-synchronization) to run as a cron job:

The `cronjob-ldap-group-sync-secure.yml` template takes the following parameters:

| Parameter                | Description                                                  |
| :----------------------- | :----------------------------------------------------------- |
| NAMESPACE                | Namespace for cron job and related resources                 |
| LDAP_URL                 | Value for group sync configuration `url`                     |
| LDAP_BIND_DN             | Value for group sync configuration `bindDN`                  |
| LDAP_BIND_PASSWORD       | Password corresponding for `bindDN`, stored in OpenShift secret |
| LDAP_CA_CERT             | LDAP certificate authority bundle content                    |
| LDAP_GROUP_UID_ATTRIBUTE | Value for `rfc2307.groupUIDAttribute`                        |
| LDAP_GROUPS_FILTER       | Value for `rfc2307.groupsQuery.filter` group sync configuration |
| LDAP_GROUPS_SEARCH_BASE  | Value for `rfc2307.groupsQuery.baseDN` group sync configuration |
| LDAP_GROUPS_WHITELIST    | Optional whitelist configuration for ldap group sync         |
| LDAP_USERS_SEARCH_BASE   | Value for `rfc2307.usersQuery.baseDN` group sync configuration |
| SCHEDULE                 | Job schedule in crontab format                               |

1. Clone the `openshift-management` project from the `redhat-cop` GitHub repository:

   ```sh
   $ git clone https://github.com/redhat-cop/openshift-management.git
   Cloning into 'openshift-management'...
   remote: Enumerating objects: 6, done.
   remote: Counting objects: 100% (6/6), done.
   remote: Compressing objects: 100% (5/5), done.
   remote: Total 216 (delta 1), reused 2 (delta 1), pack-reused 210
   Receiving objects: 100% (216/216), 54.74 KiB | 1.33 MiB/s, done.
   Resolving deltas: 100% (82/82), done.
   ```

2. Ensure that you are logged in as `system:admin`

   ```sh
   $ oc login -u system:admin
   ```

3. Create the `openshift-cluster-ops` project for group sync resources:

   ```sh
   $ oc adm new-project openshift-cluster-ops
   Created project openshift-cluster-ops
   ```

   - To create a group with a name beginning with `openshift-` requires that you use the `oc adm new-project` command.

4. Process `cronjob-ldap-group-sync-secure.yml`:

   ```sh
   $ oc process --local \
     -f openshift-management/jobs/cronjob-ldap-group-sync-secure.yml \
     -p NAMESPACE='openshift-cluster-ops' \
     -p LDAP_URL=ldap://ipa.shared.example.opentlc.com \
     -p LDAP_BIND_DN=uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com \
     -p LDAP_BIND_PASSWORD='r3dh4t1!' \
     -p LDAP_CA_CERT="$(cat ipa-ca.crt)" \
     -p LDAP_GROUP_UID_ATTRIBUTE='dn' \
     -p LDAP_GROUPS_FILTER='(objectClass=groupofnames)' \
     -p LDAP_GROUPS_SEARCH_BASE='cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com' \
     -p LDAP_GROUPS_WHITELIST="$(cat whitelist.txt)" \
     -p LDAP_USERS_SEARCH_BASE='cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com' \
     -p SCHEDULE='*/5 * * * *' \
   | oc apply -f -
   warning: --param no longer accepts comma-separated lists of values. "LDAP_BIND_DN=uid=admin,cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com" will be treated as a single key-value pair.
   warning: --param no longer accepts comma-separated lists of values. "LDAP_GROUPS_SEARCH_BASE=cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com" will be treated as a single key-value pair.
   warning: --param no longer accepts comma-separated lists of values. "LDAP_GROUPS_WHITELIST=cn=ocp-users,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\ncn=portalapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\ncn=paymentapp,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\ncn=ocp-production,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com\ncn=ocp-platform,cn=groups,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com" will be treated as a single key-value pair.
   warning: --param no longer accepts comma-separated lists of values. "LDAP_USERS_SEARCH_BASE=cn=users,cn=accounts,dc=shared,dc=example,dc=opentlc,dc=com" will be treated as a single key-value pair.
   configmap/ldap-group-sync created
   configmap/ldap-group-sync-ca created
   secret/ldap-bind-password created
   cronjob.batch/cronjob-ldap-group-sync created
   clusterrole.authorization.openshift.io/ldap-group-syncer created
   clusterrolebinding.authorization.openshift.io/system:ldap-group-syncers created
   serviceaccount/ldap-group-syncer created
   ```

   - This schedule configures the job to run once every five minutes.
   - You can safely ignore any warnings regarding comma usage.

5. Wait and then confirm that the job completes successfully:

   ```sh
   $ oc get pod -n openshift-cluster-ops
   NAME                                      READY  STATUS     RESTARTS  AGE
   cronjob-ldap-group-sync-1560364800-mn9gw  0/1    Completed  0         14s
   ```

6. Examine the log from the job run:

   ```sh
   $ oc logs cronjob-ldap-group-sync-1560364800-mn9gw -n openshift-cluster-ops
   group/ocp-users
   group/portalapp
   group/paymentapp
   group/ocp-production
   group/ocp-platform
   ```

7. This concludes this lab.



# Role-Based Access Control

## Groups and Cluster Role-Based Access Control Lab

In this lab, you configure groups and cluster role-based access control (RBAC) on your Red Hat® OpenShift® Container Platform 4 cluster.

Goals

- Create a cluster role binding to configure a group with direct `cluster-admin` access privileges
- Configure a group with `cluster-admin` access through the `sudoer` cluster role
- Restrict project self-provisioning to specific user groups
- Configure project request messages

Prerequisites

This lab assumes you completed the *Local Password and LDAP Authentication Configuration Lab* as well as the *Group Management Lab*. These two labs configure local HTPasswd and OpenTLC LDAP identity providers, local groups, and LDAP group synchronization.

## 1. Delegate Cluster Administrative Privileges

In this section, you assign `cluster-admin` access to the `local-admin` group. Because you disabled the `kubeadmin` user in a previous lab, the steps in this section must be performed using the `system:admin` user account.

### 1.1. Configure Direct `cluster-admin` Access

1. Create a cluster role binding to give `cluster-admin` rights to members of the `local-admin` group:

   ```sh
   $ oc --user=admin adm policy \
       add-cluster-role-to-group cluster-admin local-admin
   clusterrole.rbac.authorization.k8s.io/cluster-admin added: "local-admin"
   ```

2. Log in as the `alice` user to test the administrative access:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   Login successful.
   
   ... OUTPUT OMITTED ...
   ```

3. Confirm full administrative access with any verb to any resource type:

   ```sh
   $ oc auth can-i foo bar
   Warning: the server doesn't have a resource type 'bar'
   yes
   ```

### 1.2. Configure `sudoer` `cluster-admin` Access

1. Create a cluster role binding to grant `sudoer` rights to members of the `ocp-platform` group:

   ```sh
   $ oc adm policy add-cluster-role-to-group sudoer ocp-platform
   clusterrole.rbac.authorization.k8s.io/sudoer added: "ocp-platform"
   ```

2. Log in as a user that belongs to the `ocp-platform` group and confirm cluster administrative privileges:

   ```sh
   $ oc login -u david -p r3dh4t1!
   Login successful.
   
   You don't have any projects. You can try to create a new project, by running
   
       oc new-project <projectname>
   ```

3. Test direct cluster administrative access to confirm that it is not available:

   ```sh
   $ oc auth can-i foo bar
   Warning: the server doesn't have a resource type 'bar'
   no
   ```

4. Repeat the same command with the `--as=system:admin` option using the `system:admin` account:

   ```sh
   $ oc --as=system:admin auth can-i foo bar
   Warning: the server doesn't have a resource type 'bar'
   yes
   ```

### 1.3. Remove Credentials for `system:admin` from Configuration File

Now that you have delegated cluster administrative access, it is no longer appropriate to continue using TLS certificates to access `system:admin`. In this section, you reset your environment and authenticate as the `alice` user, to whom you previously delegated `cluster-admin` access.

1. Capture the cluster API URL from the current `.kube/config` configuration file:

   ```sh
   $ API_URL=$(oc whoami --show-server)
   ```

2. Remove your `kube` configuration file:

   ```sh
   $ rm -f $HOME/.kube/config
   ```

3. Log in again as the `alice` user:

   ```sh
   $ oc login -u alice -p p4ssw0rd $API_URL
   The server uses a certificate signed by an unknown authority.
   You can bypass the certificate check, but any data you send to the server could be intercepted by others.
   Use insecure connections? (y/n): y
   
   Login successful.
   
   ... output omitted ...
   ```

## 2. Restrict Access for Project Self-Provisioning

In this section, you remove the user’s default permission to create their own projects and allow only production administrators to create projects. You set a message for users who attempt to create projects without appropriate permissions. Finally, you allow users from the `ocp-production` group to create their own projects.

### 2.1. Remove OAuth Authenticated Access to Role `self-provisioner`

1. View the `self-provisioners` cluster role binding:

   ```sh
   $ oc get clusterrolebinding self-provisioners -o yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     annotations:
       rbac.authorization.kubernetes.io/autoupdate: "true"
     creationTimestamp: "2020-05-12T14:00:30Z"
     name: self-provisioners
     resourceVersion: "8347"
     selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners
     uid: 52eced3a-5fd0-4d62-87f3-93687701ec6d
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: self-provisioner
   subjects:
   - apiGroup: rbac.authorization.k8s.io
     kind: Group
     name: system:authenticated:oauth
   ```

   - This role binding has the `rbac.authorization.kubernetes.io/autoupdate: "true"` annotation.
   - Typically, you remove a cluster role binding with `oc adm policy remove-cluster-role-from-group` or `oc adm policy remove-cluster-role-from-user`. Because the `autoupdate` process restores the access, you cannot use either approach with the `self-provisioners` role binding yet.

2. Set the `rbac.authorization.kubernetes.io/autoupdate` annotation on the `self-provisioners` cluster role binding to `false`:

   ```sh
   $ oc annotate clusterrolebinding self-provisioners --overwrite \
       rbac.authorization.kubernetes.io/autoupdate=false
   clusterrolebinding.rbac.authorization.k8s.io/self-provisioners annotated
   ```

3. Remove the `system:authenticated:oauth` group from the `self-provisioners` cluster role binding:

   ```sh
   $ oc adm policy remove-cluster-role-from-group \
       self-provisioner system:authenticated:oauth
   clusterrole.rbac.authorization.k8s.io/self-provisioner removed: "system:authenticated:oauth"
   ```

4. Confirm that the `self-provisioners` cluster role binding still exists and has the `rbac.authorization.kubernetes.io/autoupdate: "false"` annotation, but no subjects:

   ```sh
   $ oc get clusterrolebinding self-provisioners -o yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     annotations:
       rbac.authorization.kubernetes.io/autoupdate: "false"
     creationTimestamp: "2020-05-12T14:00:30Z"
     name: self-provisioners
     resourceVersion: "51729"
     selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners
     uid: 52eced3a-5fd0-4d62-87f3-93687701ec6d
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: self-provisioner
   ```

### 2.2. Configure Message with Project Request Instructions

With project self-provisioning disabled, it is helpful to provide users with a message to inform them of the correct way to request a new project in OpenShift.

1. Create a file called `$HOME/projects-config.patch.json` with the following JSON patch for the project request message:

   ```texinfo
   {
     "spec": {
       "projectRequestMessage": "Please create projects using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com"
     }
   }
   ```

2. Patch the `cluster` resource of kind `projects.config.openshift.io` with the patch file:

   ```sh
   $ oc patch projects.config.openshift.io cluster --type=merge \
       -p "$(cat $HOME/projects-config.patch.json)"
   project.config.openshift.io/cluster patched
   ```

3. Log in as the non-admin `andrew` user and attempt to create a project to verify that the project request message is active:

   ```sh
   $ oc login -u andrew -p r3dh4t1!
   Login successful.
   
   You don't have any projects. Contact your system administrator to request a project.
   $ oc new-project test
   Error from server (Forbidden): Please create projects using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com
   ```

   |      | If you do not see the full error message, wait a minute or two and try again. It takes a little while for the operator to update the configuration after the patch is applied. Until the configuration has been applied you would see this error message: `Error from server (Forbidden): You may not request a new project via this API.` |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

### 2.3. Allow Production Administrators to Create Projects

In this section, you configure the `ocp-production` group that the LDAP group sync created so that its members can create projects.

1. Log in as the `alice` `cluster-admin` user:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   ... output omitted ...
   ```

2. Use `oc adm policy` again, but this time add the cluster role of `self-provisioner` to the `ocp-production` group:

   ```sh
   $ oc adm policy add-cluster-role-to-group self-provisioner ocp-production
   ```

3. Log in to the system as the `karla` user, a member of the `ocp-production` group:

   ```sh
   $ oc login -u karla -p r3dh4t1!
   Login successful.
   
   You don't have any projects. You can try to create a new project, by running
   
       oc new-project <projectname>
   ```

4. Attempt to create a project as the `karla` user and verify that this is now successful:

   ```sh
   $ oc new-project test
   Now using project "test" on server "https://api.cluster-7ae9.sandbox134.opentlc.com:6443".
   
   You can add applications to this project with the 'new-app' command. For example, try:
   
       oc new-app django-psql-example
   
   to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
   
       kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
   ```

5. Remove the test project:

   ```sh
   $ oc delete project test
   project.project.openshift.io "test" deleted
   ```

## 3. Restore Self-Provisioner Access to OAuth Authenticated Users

Completion of this section allows self-provisioning of projects to authenticated users.

|      | You must complete this section or subsequent labs will fail. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

1. Log in as the `cluster-admin` `alice` user:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   ... output omitted ...
   ```

2. Restore the `rbac.authorization.kubernetes.io/autoupdate: "true"` annotation to the self-provisioners cluster role binding:

   ```sh
   $ oc annotate clusterrolebinding self-provisioners --overwrite \
       rbac.authorization.kubernetes.io/autoupdate=true
   clusterrolebinding.rbac.authorization.k8s.io/self-provisioners annotated
   ```

3. Delete the OpenShift API server pods in the `openshift-apiserver` namespace to automatically restart them:

   ```sh
   $ oc delete pod -n openshift-apiserver --all
   pod "apiserver-cx5b8" deleted
   pod "apiserver-hc458" deleted
   pod "apiserver-rqm9n" deleted
   ```

4. Wait a short time for the API server restart to complete.

5. Verify that the self-provisioners cluster role binding reverted to including the OAuth user group:

   ```sh
   $ oc get clusterrolebinding self-provisioners -o yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     annotations:
       rbac.authorization.kubernetes.io/autoupdate: "true"
     creationTimestamp: "2020-05-12T14:00:30Z"
     name: self-provisioners
     resourceVersion: "54094"
     selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners
     uid: 52eced3a-5fd0-4d62-87f3-93687701ec6d
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: self-provisioner
   subjects:
   - apiGroup: rbac.authorization.k8s.io
     kind: Group
     name: system:authenticated:oauth
   ```

6. This concludes this lab.

   

# Service Account



## Project Role-Based Access Control Lab

In this lab, you configure project role-based access control (RBAC) on your Red Hat® OpenShift® Container Platform 4 cluster.

Goals

- Create and assign role-based access control policies and permissions
- Manage user roles to allow and restrict capabilities
- Explore Docker registry security and image pull policies
- Manage security context constraints (SCCs) to allow specific groups to run unsafe containers
- Create service accounts and assign roles to them
- Manage user, service account, and pod permissions and capabilities, using SCCs and role-based access control (RBAC), specifically roles and role bindings
- Manage user groups and permissions to simulate a developer/operations team scenario
- Troubleshoot application deployments with insufficient privileges

Prerequisites

- *Local Password Configuration Lab*
- *Group Management Lab*
- *Groups and Cluster Role-Based Access Control Lab*

|      | Your cluster is not properly configured to perform this lab unless you have completed these prerequisites. If you do not have access to that environment, stop now and complete the prerequisite labs. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 1. Create and Assign Policies and Permissions

In this section, you create projects and policies, and assign them to different groups. You also explore how to grant cluster administration roles.

You assign roles to groups in projects or the cluster according to these criteria:

| Group            | Role    | Projects                            |
| :--------------- | :------ | :---------------------------------- |
| `portalapp`      | `admin` | `portalapp-dev`, `portalapp-test`   |
| `paymentapp`     | `admin` | `paymentapp-dev`, `paymentapp-test` |
| `ocp-production` | `admin` | `portalapp-prod`, `paymentapp-prod` |

1. Log in as the `alice` user:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   Login successful.
   
   ... OUTPUT OMITTED ...
   ```

2. Create the projects for the `portalapp` and `paymentapp` applications:

   ```sh
   $ APPNAME=portalapp
   $ APPTEXT="Portal App"
   $ oc adm new-project $APPNAME-dev --display-name="$APPTEXT Development"
   Created project portalapp-dev
   $ oc adm new-project $APPNAME-test --display-name="$APPTEXT Testing"
   Created project portalapp-test
   $ oc adm new-project $APPNAME-prod --display-name="$APPTEXT Production"
   Created project portalapp-prod
   
   $ APPNAME=paymentapp
   $ APPTEXT="Payment App"
   $ oc adm new-project $APPNAME-dev --display-name="$APPTEXT Development"
   Created project paymentapp-dev
   $ oc adm new-project $APPNAME-test --display-name="$APPTEXT Testing"
   Created project paymentapp-test
   $ oc adm new-project $APPNAME-prod --display-name="$APPTEXT Production"
   Created project paymentapp-prod
   ```

   |      | `oc adm new-project` is used instead of `oc new-project` so that the `alice` user is not configured as the project requester and administrator. Using `oc adm new-project` also prevents any project request template from being applied. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. Verify that your projects are created:

   ```sh
   $ oc get projects | grep App
   paymentapp-dev                      Payment App Development   Active
   paymentapp-prod                     Payment App Production    Active
   paymentapp-test                     Payment App Testing       Active
   portalapp-dev                       Portal App Development    Active
   portalapp-prod                      Portal App Production     Active
   portalapp-test                      Portal App Testing        Active
   ```

4. Assign administrative privileges to the `portalapp` and `paymentapp` developer groups for their respective projects—in this case, using the default OpenShift `admin` cluster role:

   ```sh
   $ oc policy add-role-to-group admin portalapp -n portalapp-dev
   clusterrole.rbac.authorization.k8s.io/admin added: "portalapp"
   $ oc policy add-role-to-group admin portalapp -n portalapp-test
   clusterrole.rbac.authorization.k8s.io/admin added: "portalapp"
   $ oc policy add-role-to-group admin paymentapp -n paymentapp-dev
   clusterrole.rbac.authorization.k8s.io/admin added: "paymentapp"
   $ oc policy add-role-to-group admin paymentapp -n paymentapp-test
   clusterrole.rbac.authorization.k8s.io/admin added: "paymentapp"
   ```

5. Bind the `admin` cluster role to the administrators group on the production projects:

   ```sh
   $ oc policy add-role-to-group admin ocp-production -n portalapp-prod
   clusterrole.rbac.authorization.k8s.io/admin added: "ocp-production"
   $ oc policy add-role-to-group admin ocp-production -n paymentapp-prod
   clusterrole.rbac.authorization.k8s.io/admin added: "ocp-production"
   ```

   |      | The `admin` cluster role is assigned as a role binding. While the cluster role is global, the role bindings are created per project. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

## 2. Explore Docker Registry Security and Image Pull Policies

In this section, you explore how to set policies allowing one project to view and pull images from another project. You allow service accounts from the `paymentapp-prod` and `paymentapp-test` to pull images created in the `paymentapp-dev` project.

### 2.1. Build S2I Application

1. As the `marina` user, log in using the command line:

   ```sh
   $ oc login -u marina -p r3dh4t1!
   Login successful.
   
   You have access to the following projects and can switch between them with 'oc project <projectname>':
   
     * paymentapp-dev
       paymentapp-test
   
   Using project "paymentapp-dev".
   ```

2. Switch to project `paymentapp-dev` if it is not already selected:

   ```sh
   $ oc project paymentapp-dev
   Now using project "paymentapp-dev" on server "https://api.cluster-GUID.cluster_domain:6443".
   ```

3. Build the `sinatra` example in the `paymentapp-dev` project:

   ```sh
   $ oc new-app ruby~https://github.com/openshift/simple-openshift-sinatra-sti \
       --name=sinatra -n paymentapp-dev
   --> Found image 18a91a0 (10 days old) in image stream "openshift/ruby" under tag "2.5" for "ruby"
   ... OUTPUT OMITTED ...
   --> Creating resources ...
       imagestream.image.openshift.io "sinatra" created
       buildconfig.build.openshift.io "sinatra" created
       deploymentconfig.apps.openshift.io "sinatra" created
       service "sinatra" created
   --> Success
       Build scheduled, use 'oc logs -f bc/sinatra' to track its progress.
       Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
        'oc expose svc/sinatra'
       Run 'oc status' to view your app.
   ```

4. Wait for the build to complete:

   ```sh
   $ oc logs -f build/sinatra-1 -n paymentapp-dev
   Cloning "https://github.com/openshift/simple-openshift-sinatra-sti" ...
           Commit: d7946efc94139c60d41e974b00c8ea345284819b (Merge pull request #1 from thoraxe/3.0.0-update)
           Author: Erik M Jacobs <erikmjacobs@gmail.com>
           Date:   Fri Jul 3 19:21:33 2015 -0400
   Caching blobs under "/var/cache/blobs".
   ... OUTPUT OMITTED ...
   Writing manifest to image destination
   Storing signatures
   Successfully pushed //image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra:latest@sha256:b6c0159514873b622385a53b7912f1413b41a49b322da1e58a2f0e29759be5ab
   Push successful
   ```

   - Note that the image is placed in the `paymentapp-dev/sinatra` path in the registry corresponding to the namespace and image stream name.

5. When the application has finished building, examine the tags:

   ```sh
   $ oc describe imagestream sinatra -n paymentapp-dev
   Name:                   sinatra
   Namespace:              paymentapp-dev
   Created:                About a minute ago
   Labels:                 app=sinatra
                           app.kubernetes.io/component=sinatra
                           app.kubernetes.io/instance=sinatra
   Annotations:            openshift.io/generated-by=OpenShiftNewApp
   Image Repository:       image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra
   Image Lookup:           local=false
   Unique Images:          1
   Tags:                   1
   
   latest
     no spec tag
   
     * image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4
         4 seconds ago
   ```

6. Tag the `latest` image as `test`:

   ```sh
   $ oc tag sinatra:latest sinatra:test -n paymentapp-dev
   Tag sinatra:test set to sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4.
   ```

7. Inspect image stream tags in the `paymentapp-dev` project namespace:

   ```sh
   $ oc get imagestreamtags -n paymentapp-dev
   NAME             IMAGE REFERENCE                                                                                                                                   UPDATED
   sinatra:latest   image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4   44 seconds ago
   sinatra:test     image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4   17 seconds ago
   ```

   - The `sha256` hash of the image shows that this is the same image under both tags. Your `sha256` value will be different from what is shown here.

### 2.2. Attempt Project Deployment

In this section, you attempt to deploy the image in the `paymentapp-test` project. The command is expected to fail due to lack of permissions.

1. Use `oc new-app` to deploy the `paymentapp-dev` project’s `sinatra` image with the `test` tag:

   ```sh
   $ oc new-app paymentapp-dev/sinatra:test -n paymentapp-test
   --> Found image 0e6a03b (14 minutes old) in image stream "paymentapp-dev/sinatra" under tag "test" for "paymentapp-dev/sinatra:test"
   ... OUTPUT OMITTED ...
   --> Creating resources ...
       imagestreamtag.image.openshift.io "sinatra:test" created
       deploymentconfig.apps.openshift.io "sinatra" created
       service "sinatra" created
   --> Success
       Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
        'oc expose svc/sinatra'
       Run 'oc status' to view your app.
   ```

2. Note that the deployment fails due to the inability to pull the image:

   ```sh
   $ oc get pods -n paymentapp-test
   NAME               READY   STATUS         RESTARTS   AGE
   sinatra-1-2tg8r    0/1     ErrImagePull   0          43s
   sinatra-1-deploy   1/1     Running        0          52s
   ```

   - You may find the test deployment succeeds if the pod is scheduled on the node on which the build occurred. This occurs because this node already has the image locally.

### 2.3. Grant Image Pull Rights

1. Grant image pull rights to the service accounts in the `paymentapp-prod` and `paymentapp-test` projects on the `paymentapp-dev` project:

   ```sh
   $ oc policy add-role-to-group -n paymentapp-dev \
       system:image-puller system:serviceaccounts:paymentapp-prod
   clusterrole.rbac.authorization.k8s.io/system:image-puller added: "system:serviceaccounts:paymentapp-prod"
   $ oc policy add-role-to-group -n paymentapp-dev \
       system:image-puller system:serviceaccounts:paymentapp-test
   clusterrole.rbac.authorization.k8s.io/system:image-puller added: "system:serviceaccounts:paymentapp-test"
   ```

2. Assign the `registry-viewer` role to the `ocp-production` group so that the production administrators can view the image streams:

   ```sh
   $ oc policy add-role-to-group registry-viewer ocp-production -n  paymentapp-dev
   clusterrole.rbac.authorization.k8s.io/registry-viewer added: "ocp-production"
   ```

3. As the `marina` user, restart the build for the `sinatra` application in the `paymentapp-test` project:

   ```sh
   $ oc rollout latest dc/sinatra -n paymentapp-test
   deploymentconfig.apps.openshift.io/sinatra rolled out
   ```

   - Note that `latest` refers to the latest version of the DeploymentConfig. The image tag to be used is still `test`.

4. After changing the credentials of the service account, verify that the deployment is successful:

   ```sh
   $ oc get pods -n paymentapp-test
   NAME               READY   STATUS      RESTARTS   AGE
   sinatra-1-deploy   0/1     Completed   0          11m
   sinatra-2-deploy   0/1     Completed   0          101s
   sinatra-2-ksscl    1/1     Running     0          92s
   ```

5. If the deployment is successful, assume that you tested the application and it passed, then tag the image as `sinatra:prod` and deploy it to the `paymentapp-prod` project:

   ```sh
   $ oc tag sinatra:test sinatra:prod -n paymentapp-dev
   Tag sinatra:prod set to sinatra@sha256:b6c0159514873b622385a53b7912f1413b41a49b322da1e58a2f0e29759be5ab.
   ```

6. Again inspect image stream tags in the `paymentapp-dev` project namespace:

   ```sh
   $ oc get imagestreamtags -n paymentapp-dev
   NAME             IMAGE REFERENCE                                                                                                                                   UPDATED
   sinatra:latest   image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4   3 minutes ago
   sinatra:prod     image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4   9 seconds ago
   sinatra:test     image-registry.openshift-image-registry.svc:5000/paymentapp-dev/sinatra@sha256:4cffbbc76a25e69467ac224f43ed4ed6cdacf99ec5720c660adf613528732dc4   3 minutes ago
   ```

7. Log in as the `karla` user, a member of the `ocp-production` group, and deploy the `prod` tag image in the `paymentapp-prod` project:

   ```sh
   $ oc login -u karla -p r3dh4t1!
   Login successful.
   
   You have access to the following projects and can switch between them with 'oc project <projectname>':
   
     * paymentapp-dev
       paymentapp-prod
       portalapp-prod
   
   Using project "paymentapp-dev".
   
   $ oc new-app paymentapp-dev/sinatra:prod -n paymentapp-prod
   --> Found image 0e6a03b (32 minutes old) in image stream "paymentapp-dev/sinatra" under tag "prod" for "paymentapp-dev/sinatra:prod"
   ... OUTPUT OMITTED ...
   --> Creating resources ...
       imagestreamtag.image.openshift.io "sinatra:prod" created
       deploymentconfig.apps.openshift.io "sinatra" created
       service "sinatra" created
   --> Success
       Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
        'oc expose svc/sinatra'
       Run 'oc status' to view your app.
   ```

## 3. Allow Production Administrators to Run Unsafe Containers

In this section, you allow one of the projects to create and deploy an S2I-built image with `root` permissions—in other words, to run privileged containers.

Users generally do not create pods directly. They create a deployment configuration or a replication controller to launch the pods. Therefore, it is the `serviceaccount` in the project that must be assigned the permissions.

OpenShift Container Platform comes with a number of security context constraints (SCCs). The `anyuid` SCC does what you want—it allows you to run containers as any UID, specifically `root`. Because only production administrators have access to the production projects, you can simply allow the service accounts for the production projects to run containers as any UID.

### 3.1. Create Privileged Image that Runs as `root`

In this section, you use SCCs to allow service accounts in the `mygitlab-prod` project to run images and containers running as the `root` user.

First, you attempt to deploy the `Gitlab-ce` application and note how it fails when it cannot run as `root` in the container. Then you provide the permission to run the image and deploy it again.

1. Log in as the `andrew` user and create a project called `mygitlab`:

   ```sh
   $ oc login -u andrew -p r3dh4t1!
   Login successful.
   
   You have access to the following projects and can switch between them with 'oc project <projectname>':
   
     * portalapp-dev
       portalapp-test
   
   Using project "portalapp-dev".
   
   $ oc new-project mygitlab
   Now using project "mygitlab" on server "https://api.cluster-edad.sandbox338.opentlc.com:6443".
   
   ... OUTPUT OMITTED ...
   ```

2. Deploy the `gitlab-ce` application from Docker Hub:

   ```sh
   $ oc new-app docker.io/gitlab/gitlab-ce:12.10.3-ce.0 --name gitlab-ce
   --> Found container image f2e4872 (8 days old) from docker.io for "docker.io/gitlab/gitlab-ce:12.10.3-ce.0"
   
       * An image stream tag will be created as "gitlab-ce:12.10.3-ce.0" that will track this image
       * This image will be deployed in deployment config "gitlab-ce"
       * Ports 22/tcp, 443/tcp, 80/tcp will be load balanced by service "gitlab-ce"
         * Other containers can access this service through the hostname "gitlab-ce"
       * This image declares volumes and will default to use non-persistent, host-local storage.
         You can add persistent volumes later by running oc set volume dc/gitlab-ce --add ...
       * WARNING: Image "docker.io/gitlab/gitlab-ce:12.10.3-ce.0" runs as the root user which may not be permitted by your cluster administrator
   
   --> Creating resources ...
       imagestream.image.openshift.io "gitlab-ce" created
       deploymentconfig.apps.openshift.io "gitlab-ce" created
       service "gitlab-ce" created
   --> Success
       Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
        oc expose svc/gitlab-ce
       Run oc status to view your app.
   ```

   - It can take a few minutes to pull down the image, but expect the deployed pod to fail.

3. In the logs, verify that the container did not start successfully because of permissions issues:

   ```sh
   $ oc logs -f dc/gitlab-ce
   Thank you for using GitLab Docker Image!
   Current version: gitlab-ce=12.10.3-ce.0
   ... OUTPUT OMITTED ...
   Preparing services...
   ln: failed to create symbolic link '/opt/gitlab/service/sshd': Permission denied
   ```

4. As the `alice` user, modify the SCCs to allow the `anyuid` privilege for the `default` service account in the `mygitlab-prod` project:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   Login successful.
   
   ... OUTPUT OMITTED ...
   $ oc adm policy add-scc-to-user anyuid system:serviceaccount:mygitlab:default
   securitycontextconstraints.security.openshift.io/anyuid added to: ["system:serviceaccount:mygitlab:default"]
   ```

   |      | The `default` service account is used if no specific service account is specified for an application. Because granting permissions to the `default` service account grants permissions to *every* pod in that project, it is strongly recommended to create a dedicated service account for an application that requires elevated privileges. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

### 3.2. Run Privileged Image that Runs as `root`

In this section, you verify that an unprivileged service account, `andrew`, can deploy a container that runs as the `root` user.

1. Now that `service account` has permissions, switch back to the `andrew` user, restart the deployment, and note the changes:

   ```sh
   $ oc login -u andrew
   Logged into "https://api.cluster-edad.sandbox338.opentlc.com:6443" as "andrew" using existing credentials.
   
   You have access to the following projects and can switch between them with 'oc project <projectname>':
   
     * mygitlab
       portalapp-dev
       portalapp-test
   
   Using project "mygitlab".
   $ oc rollout latest dc/gitlab-ce -n mygitlab
   deploymentconfig.apps.openshift.io/gitlab-ce rolled out
   ```

2. When the pod is running (this can take a couple of minutes), use `oc rsh` to connect to the pod (as shown), or use the web UI:

   ```sh
   $ oc rsh dc/gitlab-ce
   #
   ```

3. When you are connected to the pod, issue the `whoami` and `id` commands to verify that you are `root`, then type `exit` to close the container shell:

   ```sh
   # whoami
   root
   # id
   uid=0(root) gid=0(root) groups=0(root)
   # exit
   ```

### 3.3. Expose and Test Service

In this section, you expose the service as a route and examine it from your browser.

1. Determine the name of the `gitlab-ce` service:

   ```sh
   $ oc get services
   NAME       TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)                AGE
   gitlab-ce  ClusterIP  172.30.74.85  <none>       22/TCP,80/TCP,443/TCP  15m
   ```

   - Note that the service has multiple available ports.

2. Expose port 80 of the `gitlab` service as a route:

   ```sh
   $ oc expose service gitlab-ce --port 80
   route.route.openshift.io/gitlab-ce exposed
   ```

3. Verify the name of the route created for the service:

   ```sh
   $ oc get route
   NAME       HOST/PORT                                                    PATH  SERVICES   PORT  TERMINATION  WILDCARD
   gitlab-ce  gitlab-ce-mygitlab.apps.cluster-GUID.basedomain            gitlab-ce  80                 None
   ```

4. From your local browser, attempt to reach your application at the host shown in the output.

## 4. Clean Up Lab Environment

In this section, you clean up the lab environment.

1. Log in as the `alice` user:

   ```sh
   $ oc login -u alice -p p4ssw0rd
   Login successful.
   
   ... OUTPUT OMITTED ...
   ```

2. Delete the `mygitlab` project:

   ```sh
   $ oc delete project mygitlab
   project.project.openshift.io "mygitlab" deleted
   ```

3. Remove the `anyuid` SCC from the `system:serviceaccount:mygitlab:default` service account:

   ```sh
   $ oc adm policy remove-scc-from-user anyuid system:serviceaccount:mygitlab:default
   securitycontextconstraints.security.openshift.io/anyuid removed from: ["system:serviceaccount:mygitlab:default"]
   ```

4. This concludes this lab.



# Resource Management



## Resource Management Lab

In this lab, you learn how to manage Red Hat® OpenShift® Container Platform resources.

Goals

- Manage resource quotas
- Manage limit ranges
- Influence quality-of-service (QoS) classes
- Create a default project request template

Prerequisites

- Completion of the *Local Password and LDAP Authentication Configuration Lab*

  If you have not completed the prerequisite lab, complete it now. Otherwise, your environment is not properly configured with the correct user to perform this lab.

|      | The web console is completely redesigned for OpenShift Container Platform 4 and incorporates both the developer and operator experience into the same UI. Many steps in this lab involve using the web UI. Some steps require the command line interface and include specific command-line instructions. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 1. Manage Resource Quotas

### 1.1. Create Project

1. At the command line, retrieve your web console URL:

   ```texinfo
   $ oc whoami --show-console
   https://console-openshift-console.apps.cluster-GUID.basedomain
   ```

2. From a web browser, use the URL with `OPENTLC LDAP` as the authentication provider and the following credentials to log in:

   - **Username**: `andrew`

   - **Password**: `r3dh4t1!`

     |      | The first time you access the URL, you may need to accept one or more self-signed certificates. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

3. From the web UI, navigate to **Home → Projects** in the menu on the left and select **Create Project** to create a new project.

4. Create the new project using the following information:

   - **Name**: `resourcemanagement`
   - **Display Name**: `Resource Management`
   - **Description**: `This is the project we use to learn about resource management.`

5. Click **Create**.

6. In the menu on the left, navigate to **Workloads → Pods**.

   - Note that no pods are currently running in this project.

### 1.2. Apply Resource Quota to Project

1. From the web UI, navigate to **Administration → Resource Quotas**.

   - There are currently no resource quotas defined for the `resourcemanagement` project.
   - Note that there are no options to create any quotas because the user `andrew` does not have `cluster-admin` permissions.

2. In your VM, make sure that you are logged in as the `system:admin` user:

   ```sh
   $ export KUBECONFIG=$HOME/cluster-$GUID/auth/kubeconfig
   $ oc login -u system:admin
   ```

3. Create the resource quota for the `resourcemanagement` project:

   ```sh
   $ echo '---
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: test-quota
   spec:
     hard:
       requests.memory: 512Mi
       requests.cpu: "1"
       pods: "3"
       services: "5"
       replicationcontrollers: "5"
       resourcequotas: "1"' | oc create -n resourcemanagement -f - 
   
   resourcequota/test-quota created
   ```

4. In the web UI as the `andrew` user, navigate to **Administration → Resource Quotas** and select the `test-quota` resource quota to verify that the quota exists.

   - The resource quotas details pane shows that this project is not using any of its resources, and therefore has resources available before it exceeds its resource quota.

5. Using the menu on the left, navigate to **Workloads → Deployments**, then click **Create Deployment**.

   - Examine the default deployment that is presented in an editor pane and note what actions it performs:

     |      | Do not copy the following deployment content into an editor because the callout characters generate errors. The content is present by default when you click **Create Deployment**. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: example 
       namespace: resourcemanagement
     spec:
       selector:
         matchLabels:
           app: hello-openshift
       replicas: 3 
       template:
         metadata:
           labels:
             app: hello-openshift
         spec:
           containers:
             - name: hello-openshift
               image: openshift/hello-openshift 
               ports:
                 - containerPort: 8080
     ```

     |      | Creates an application named `example`               |
     | ---- | ---------------------------------------------------- |
     |      | Deploys three replicas                               |
     |      | Uses the `openshift/hello-openshift` container image |

6. Click **Create** and observe the **Details** tab.

   - According to this view, no pods are being deployed.

7. Select the **Events** tab to help identify the problem. You see that the ReplicaSet has been scaled to three copies.

8. Navigate to to **Workloads → Replica Sets** for this project.

9. Select your `ReplicaSet` and navigate to the **Events** tab to discover the problem:

   - According to the events displayed, you have a resource quota assigned to this project, but your new pods did not specify any CPU or memory requests.

     ```
     (combined from similar events): Error creating: pods "example-75778c488-sjt9r" is forbidden: failed quota: test-quota: must specify requests.cpu,requests.memory
     ```

## 2. Manage Limit Ranges

For resource quotas to be effective, you must have *requests* and *limits* defined for all pods in the project. This is necessary so that OpenShift can evaluate what resources a pod requests against the resource quota assigned to the project. Using a limit range is the best way to make sure that every pod is required to define the requests and limits for its containers, and to assign defaults if none are provided in the pod `spec`.

### 2.1. Apply Limit Range to Project

In this section, you use your client VM to create a limit range for your project, because creating a `LimitRange` object requires `cluster-admin` level permissions.

1. Make sure that you are logged in as the `system:admin` user:

   ```sh
   $ oc login -u system:admin
   ```

2. Create a limit range for the `resourcemanagement` project:

   ```sh
   $ echo '---
   kind: LimitRange
   apiVersion: v1
   metadata:
     name: limits
   spec:
     limits:
     - type: Pod
       max:
         cpu: 1
         memory: 1.5Gi
       min:
         cpu: 100m
         memory: 50Mi
     - type: Container
       max:
         cpu: 500m
         memory: 750Mi
       min:
         cpu: 100m
         memory: 50Mi
       default:
         cpu: 200m
         memory: 100Mi' | oc create -n resourcemanagement -f - 
   ```

   |      | It is a best practice to define only a single `LimitRange` object per project to avoid conflicts. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. In the web UI, make sure that you are logged in as the `andrew` user.

4. Navigate to **Adminstration → Limit Ranges** and select the `limits` `LimitRange` object.

   - Now that there are default requests and limits defined, expect the new `example` application to begin rolling out when redeployed.

5. In the web UI, navigate to **Workloads → Replica Sets**, find your ReplicaSet and select the three dots on the right. Then select **Delete ReplicaSet** to force the Deployment to recreate the ReplicaSet. Click **Delete** in the prompt that appears.

6. You will see that a new ReplicaSet immediately gets created - and it immediately creates three pods.

7. Select your ReplicaSet, then select the **Pods** tab.

   - Expect to see that there are three pods running.

### 2.2. Test Pod Count Limits

Now that there is an application deployed and running three replicas, you scale it up to a point where it exceeds the resource quota defined for the number of pods. If you look back to the resource quota you defined earlier in this lab, you notice that it is not just the total count of pods that the resource quota restricts, but also the amount of memory and the number of CPUs, services, and replication controllers.

1. In the web UI, make sure that you are logged in as the `andrew` user.

2. Using the menu on the left, navigate to **Workloads → Replica Sets**, and select the `example` replica set.

3. Select **Actions → Edit Replica Set** to scale the ReplicaSet up.

4. In the online editor that displays, change the number of `replicas` to `4` and click **Save**:

5. Navigate to the **Workloads → Pods** menu.

   - Expect to see that there are still three pods deployed. This is because the modified replica set is controlled by the deployment that you created earlier.
   - If you need to scale up the number of replicas, the change must be made to the deployment or it is overwritten.

6. Navigate to the **Workloads → Deployments** menu and select your `example` deployment.

7. Click the `up` arrow next to the circle to scale to 4 pods. You will see a message `scaling to 4` in the circle - but it never seems to create the fourth pod.

8. Navigate to the **Workloads → Pods** menu.

   - Note that you still see only three pods.

9. Follow the same path as before and navigate to the **Workloads → Replica Set** menu, select your `example` replica set, then select the **Events** tab.

   - Expect to see an error message showing you that the number of pods for this project was exceeded:

     ```
     (combined from similar events): Error creating: pods "example-75778c488-7gh5j" is forbidden: exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3
     ```

10. Navigate to the **Workloads → Deployments** menu and scale the number of pods in your deployment down to `1`.

### 2.3. Test Pod Resource Limits

The *resource quota* defined earlier also controls the CPU and memory resources that the pods in this project can request. You purposely add larger CPU request sizes to the pod specifications to run out of the `cpu.requests` resource quota before you exceed the number of pods allowed.

1. While still in your `example` deployment, click the **YAML** tab.

2. Find the empty `resources` definition in line 30 and replace it with the following, and then click **Save**:

   ```yaml
             resources:
               limits:
                 cpu: 500m
               requests:
                 cpu: 500m
   ```

   |      | You must observe proper YAML formatting when making this kind of change. Make sure to get the indentation right! |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. Examine the example below to verify that you added the YAML code in the correct location:

   ```
   [...]
       spec:
         containers:
           - name: hello-openshift
             image: openshift/hello-openshift
             ports:
               - containerPort: 8080
                 protocol: TCP
             resources:
               limits:
                 cpu: 500m
               requests:
                 cpu: 500m
             terminationMessagePath: /dev/termination-log
             terminationMessagePolicy: File
             imagePullPolicy: Always
   [...]
   ```

   - When the deployment is updated with the new pod specifications, a new replica set is created with a new pod deployed.

4. Navigate to the **Workloads → Pods** menu and select your single pod.

5. Select the **YAML** tab and note the new *resources* that you defined in the parent deployment:

   ```sh
   [...]
     containers:
       - resources:
           limits:
             cpu: 500m
             memory: 100Mi
           requests:
             cpu: 500m
             memory: 100Mi
         terminationMessagePath: /dev/termination-log
   [...]
   ```

6. Alternatively, click the `hello-openshift` container while in the **Pod** view to see these resources and more information.

7. Navigate to **Workloads → Deployments**, select your deployment, and scale it up to `3`.

8. Navigate to **Workloads → Pods**.

   - Expect to see only two new pods started.

9. Navigate to **Workloads → Replica Sets**, select the `example` replica set (the one that says `2` of `3` pods are running in the **Status** column), and click **Events**.

10. Select the **Events** tab to see that the new CPU requests defined for the pods exceed the allocated CPU request quota:

    ```
    (combined from similar events): Error creating: pods "example-54c95fdcd8-fn6l8" is forbidden: exceeded quota: test-quota, requested: requests.cpu=500m, used: requests.cpu=1, limited: requests.cpu=1
    ```

11. Navigate to **Administration → Resource Quotas** and select `test-quota`.

    - The UI shows you the current usage rather than the allocated resources.
    - In this case, even though the request for the *number* of pods has not exceeded the quota, the *amount* of CPU requests was exceeded.

12. Navigate to **Workloads → Deployments** and scale your application back down to `1` replica.

### 2.4. Influence Quality of Service (QoS) Classes

OpenShift automatically assigns a QoS to every pod that is deployed. The user does not directly define or modify the QoS, but rather selects the class by providing the appropriate requests and limits value for CPU or memory resources. The QoS class assigned to a pod influences the scheduler in how it determines pod priority and the node location to run the workload.

The QoS classes are as follows:

| Class Name   | Requirements                                                 |
| :----------- | :----------------------------------------------------------- |
| `BestEffort` | Applied when container requests and limits *not* defined     |
| `Burstable`  | Applied when container requests defined but limits not defined |
| `Guaranteed` | Applied when container requests and limits defined with *same* value |

In this section, you influence the QoS class assigned to the pods in your application.

|      | Make sure you scaled your deployment back down to `1` replica in the previous section. You can do this or verify this by navigating to **Workloads → Deployments**. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

1. Use the existing application that is deployed in this lab and navigate to **Workloads → Pods**, then select the single pod currently running.

2. Select the **YAML** tab.

3. At the bottom of the page, under the `status` section, note that the current `qosClass` is defined as `Guaranteed`:

   - This is because earlier in the lab, you set the CPU `requests` and `limits` to the same value.

4. Navigate to **Workloads → Deployments** and select the `example` deployment.

5. Select the **YAML** tab, find the `resources` section, then remove the CPU limits and change the CPU requests to `100m`.

6. Verify that your YAML file looks like this after the changes:

   ```
   [...]
         containers:
           - name: hello-openshift
             image: openshift/hello-openshift
             ports:
               - containerPort: 8080
                 protocol: TCP
             resources:
               requests:
                 cpu: 100m
             terminationMessagePath: /dev/termination-log
   [...]
   ```

7. Navigate to **Workloads → Pods** and select the single pod currently running.

8. Select the **YAML** tab.

9. At the bottom of this page, under the `status` section, note that the `qosClass` is now defined as `Burstable`:

   - This is because you now have a CPU *request* with a smaller value than the CPU *limit*.

## 3. Define Default Project Request Template

To ensure that all of the projects have a basic limit range and resource quota defined, you can define a *default* project request template. With this in place, all new projects created in the cluster by any user have these objects defined and applied.

### 3.1. Create Project Request Template

1. On the client VM, make sure that you are logged in as the `system:admin` user:

   ```sh
   $ oc login -u system:admin --kubeconfig=$HOME/cluster-$GUID/auth/kubeconfig
   ```

2. Create a local `template` file to modify:

   ```sh
   $ oc adm create-bootstrap-project-template -o yaml > $HOME/project_request_template.yaml
   ```

   - The template includes some basic objects needed by every project.

3. Still on the client VM, open the `$HOME/project_request_template.yaml` file in an editor and add the following YAML code to the list of objects:

   ```yaml
   - apiVersion: "v1"
     kind: "LimitRange"
     metadata:
       name: "core-resource-limits"
     spec:
       limits:
         - type: "Pod"
           max:
             cpu: "1"
             memory: "1Gi"
           min:
             cpu: "200m"
             memory: "6Mi"
         - type: "Container"
           max:
             cpu: "1"
             memory: "1Gi"
           min:
             cpu: "100m"
             memory: "4Mi"
           default:
             cpu: "300m"
             memory: "200Mi"
           defaultRequest:
             cpu: "200m"
             memory: "100Mi"
           maxLimitRequestRatio:
             cpu: "10"
   - apiVersion: v1
     kind: ResourceQuota
     metadata:
       name: default-quota
     spec:
       hard:
         pods: "4"
         requests.cpu: "1"
         requests.memory: 1Gi
         limits.cpu: "2"
         limits.memory: 2Gi
         persistentvolumeclaims: "10"
         services: "100"
         services.loadbalancers: "5"
   ```

   |      | Make sure that you add this to the correct location in the template—at the end of the `objects` section and before the `parameters` section. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

4. Save your modified YAML file.

5. Create the new `project-request` template in the `openshift-config` namespace:

   ```sh
   $ oc create -f $HOME/project_request_template.yaml -n openshift-config
   ```

### 3.2. Apply Project Request Template

In OpenShift Container Platform 4, rather than editing a configuration file on master servers directly, you must modify the cluster project configuration custom resource. Adding this to the `spec` section of the custom resource is all that is necessary.

1. Retrieve the details of the custom resource (CR) project configuration:

   ```texinfo
   $ oc get projects.config.openshift.io -o yaml
   apiVersion: v1
   items:
   - apiVersion: config.openshift.io/v1
     kind: Project
     metadata:
       annotations:
         release.openshift.io/create-only: "true"
       creationTimestamp: "2019-06-07T17:33:55Z"
       generation: 2
       name: cluster
       resourceVersion: "29150"
       selfLink: /apis/config.openshift.io/v1/projects/cluster
       uid: 6c9d9883-894a-11e9-9e52-02e747f7e3c4
     spec:
       projectRequestMessage: Please create projects using the portal http://portal.company.internal/provision
         or PaaS Support at paas-support@example.com
   kind: List
   metadata:
     resourceVersion: ""
     selfLink: ""
   ```

   - At present, it does not have a project request template defined in the `spec` section.

2. Run the following command to modify the CR, adding a reference to the *template* that you created earlier in the lab:

   ```sh
   $ oc patch projects.config.openshift.io cluster -p '{"spec": {"projectRequestTemplate": {"name": "project-request"}}}' --type=merge
   ```

3. Examine the configuration again to verify that the changes are in effect:

   ```texinfo
   $ oc get projects.config.openshift.io -o yaml
   apiVersion: v1
   items:
   - apiVersion: config.openshift.io/v1
     kind: Project
     metadata:
       annotations:
         release.openshift.io/create-only: "true"
       creationTimestamp: "2019-07-07T21:27:44Z"
       generation: 6
       name: cluster
       resourceVersion: "419692"
       selfLink: /apis/config.openshift.io/v1/projects/cluster
       uid: 0ec6a2cc-a0fe-11e9-80e7-020dd75f36a4
     spec:
       projectRequestMessage: Please create projects using the portal http://portal.company.internal/provision
         or PaaS Support at paas-support@example.com
       projectRequestTemplate:
         name: project-request
   kind: List
   metadata:
     resourceVersion: ""
     selfLink: ""
   ```

4. In the web UI as the `andrew` user, navigate to **Home → Projects** and create a new project called `templatetesting`.

5. Navigate to **Administration → Resource Quotas** and see the new `default-quota` object that was created with the project.

6. Navigate to **Administration → Limit Ranges** and see the new `core-resource-limits` object that was created with the project.

|      | If you do not see the Resource Quota or Limit Range wait a few minutes and try again. The operator needs a few seconds to make the updates to the configuration. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

